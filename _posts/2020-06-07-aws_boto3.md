---
title: "Managing AWS s3 buckets from CMD using CLI and Jupyter Notebook using boto3"
date: 2020-05-31
categories: [Python]
tags: [AWS]
header:
  image: "/images/stocks_fbprophet/header.gif"
excerpt: "--"
mathjax: "true"
---
## AWS, CLI, Boto3, and AWS S3:
*Objective of this post is to make acquintance with AWS s3 services and how to access it and manage it with CLI and Boto3 SDK for python.*


### Lets get these key terms explained for once and for all:

**What is AWS?**
Amazon Web Services is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis.

**What is AWS S3?**
Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services that provides object storage through a web service interface. Amazon S3 uses the same scalable storage infrastructure that Amazon.com uses to run its global e-commerce network.

**What is AWS CLI?**
The AWS Command Line Interface (CLI) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.

**What is Boto3?**
Boto is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services, such as EC2 and S3. Boto provides an easy to use, object-oriented API, as well as low-level access to AWS services.

Boto3 is built on the top of a library called Botocore, which is shared by the AWS CLI. Botocore provides the low level clients, session and credentials and configuration data. Boto3 built on the top of Botocore by providing its own session, resources, and collections.


### Steps to making best use of CLI:
- Lets make 2 buckets
- Upload some files
- Copy file from one bucket to another
- Download some files from these buckets
- Sync them with a local folder
- Make a presign
- Delete these buckets

```
    C:\Users\shrey>aws configure

    AWS Access Key ID [****************TM36]: ****2O
    AWS Secret Access Key [****************tmuq]: /*************1
    Default region name [us-east-1]: us-east-1
    Default output format [JSON]: JSON
```
**Lets make 2 buckets:**

```
C:\Users\shrey>aws s3 mb s3://sjk714
make_bucket: sjk714

C:\Users\shrey>aws s3 mb s3://sjk714a
make_bucket: sjk714a
```

**Lets copy some files to the bucket sjk714:**
- Remember the first location is always the Source and second location is Destination

```
C:\Users\shrey>aws s3 cp C:\Users\shrey\OneDrive\Desktop\s3\sjk.txt s3://sjk714
upload: OneDrive\Desktop\s3\sjk.txt to s3://sjk714/sjk.txt
```

**Lets copy the uploaded file in sjk714 bucket to sjk714a bucket:**

```
C:\Users\shrey>aws s3 cp s3://sjk714/sjk.txt s3://sjk714a
copy: s3://sjk714/sjk.txt to s3://sjk714a/sjk.txt
```

**Lets move file from sjk714 bucket to local drive:**
- Remember moving file will remove it from source location and move it to destination

```
C:\Users\shrey>aws s3 mv s3://sjk714/sjk.txt C:\Users\shrey\OneDrive\Desktop\s3
move: s3://sjk714/sjk.txt to OneDrive\Desktop\s3\sjk.txt
```
You will notice that the file from sjk714 bucket was removed with the above line.

**Lets use sync to sync files from my local destination to sjk714 bucket:**
- First we will list down the files in s3 bucket.
- Next we will sync
- Now we will again check the files in s3 bucket.

```
# sjk714 bucket is empty
C:\Users\shrey>aws s3 ls s3://sjk714

C:\Users\shrey>aws s3 ls s3://sjk714a
2020-06-07 17:28:28         13 sjk.txt

C:\Users\shrey>:: we can see that sjk714 is empty
C:\Users\shrey>aws s3 sync C:\Users\shrey\OneDrive\Desktop\s3 s3://sjk714
upload: OneDrive\Desktop\s3\sjk.txt to s3://sjk714/sjk.txt
upload: OneDrive\Desktop\s3\sjk1.txt to s3://sjk714/sjk1.txt
upload: OneDrive\Desktop\s3\sjk2.txt to s3://sjk714/sjk2.txt
upload: OneDrive\Desktop\s3\down\sjk2.txt to s3://sjk714/down/sjk2.txt

C:\Users\shrey>aws s3 ls s3://sjk714
                           PRE down/
2020-06-07 17:36:56         13 sjk.txt
2020-06-07 17:36:56         12 sjk1.txt
2020-06-07 17:36:56         14 sjk2.txt

# all the files from local destination were moved to the bucket
```
**The cool thing about sync function:**
- If you delete one file from the s3 bucket and sync again with the local destination it will only upload the deleted files.
- If the files were deleted from the source destination, AWS wont delete files from bucket without `--delete` extension.

**Lets make a presign to access the file data in the Buckets:**
- It gives you a link that will show the content of your files.
- The `--expires-in 30` significe that the link will be valid for 30 seconds.

```
C:\Users\shrey>aws s3 presign s3://sjk714/sjk.txt --expires-in 30
https://sjk714.s3.amazonaws.com/sjk.txt?AWSAccessKeyId=AKIAQU6F5LRCF2EAES2O&Signature=yQpayn1kBJqYo6BCW488AwwQnAQ%3D&Expires=1591566243
```
### Cleaning up post practice:
- Always remember to delete all the buckets to avoid any charges.
- You have to empty the bucket before you can delete it.

```
C:\Users\shrey>aws s3 rm s3://sjk714 --recursive
delete: s3://sjk714/down/sjk2.txt
delete: s3://sjk714/sjk1.txt
delete: s3://sjk714/sjk.txt
delete: s3://sjk714/sjk2.txt

C:\Users\shrey>aws s3 rm s3://sjk714a --recursive
delete: s3://sjk714a/sjk.txt

C:\Users\shrey>aws s3 rb s3://sjk714a
remove_bucket: sjk714a

C:\Users\shrey>aws s3 rb s3://sjk714
remove_bucket: sjk714
```
