---
title: "AWS S3 File Handling and Data Manipulation using Boto3 on Python."
date: 2020-06-17
categories: [Python]
tags: [AWS]
header:
  image: "/images/aws_s3_boto3/header.png"
excerpt: "Boto3 has widespread of methods and functionalities that are simple yet incredibly powerful. The objective of this notebook was to successfully make S3 Buckets, upload files to
it, made data modifications and discover ways to access private objects in the S3 buckets all this using python script with the help on Boto3"
mathjax: "true"
---

## INTRODUCTION TO AWS S3 USING BOTO3 IN PYTHON



**What is Boto3?**

Boto is the Amazon Web Services (AWS) SDK for Python. It enables Python developers to create, configure, and manage AWS services, such as EC2 and S3. Boto provides an easy to use, object-oriented API, as well as low-level access to AWS services.

Boto3 is built on the top of a library called Botocore, which is shared by the AWS CLI. Botocore provides the low level clients, session and credentials and configuration data. Boto3 built on the top of Botocore by providing its own session, resources, and collections.

**We will cover AWS S3 services in this notebook and how to access data and manipulate them using Boto3**


### What we will learn with this notbook?
- Make and delete Buckets
- Upload, Copy, Delete Objects within buckets
- Make security changes and ways to access the object from a Private Buckets


```python
# Import Library
import boto3

# Build a client to access the service methods
s3 = boto3.client('s3',region_name='us-east-1',
                  aws_access_key_id='****************',
                  aws_secret_access_key='****************')

```


```python
#List down the existing buckets
response = s3.list_buckets()
```

**The above code has empty output because we have no buckets created yet**
### Create S3 Buckets:


```python
#Create Bucket
s3.create_bucket(Bucket='example1811')
```




    {'ResponseMetadata': {'RequestId': '26151D4CBA49A11F',
      'HostId': 'GVFVb+AOuMcG3eFuhcrgoqCIMhjz/LK4G9jE1F2UxNiky3Yl33FBWklwOt6J4wal8vsa38fgMLs=',
      'HTTPStatusCode': 200,
      'HTTPHeaders': {'x-amz-id-2': 'GVFVb+AOuMcG3eFuhcrgoqCIMhjz/LK4G9jE1F2UxNiky3Yl33FBWklwOt6J4wal8vsa38fgMLs=',
       'x-amz-request-id': '26151D4CBA49A11F',
       'date': 'Tue, 16 Jun 2020 23:35:19 GMT',
       'location': '/example1811',
       'content-length': '0',
       'server': 'AmazonS3'},
      'RetryAttempts': 0},
     'Location': '/example1811'}




```python
s3.list_buckets()['Buckets']
```




    [{'Name': 'example1811',
      'CreationDate': datetime.datetime(2020, 6, 16, 23, 35, 19, tzinfo=tzutc())}]




```python
response = s3.delete_bucket(Bucket = 'example1811')
```


```python
s3.list_buckets()['Buckets']
```




    []



### Uploading Files:

We will use `upload_file` method.
It takes in 3 primary parameters:
- **Filename:** location of the file
- **Bucket:** Name of the Bucket you want to upload the file to
- **Key:** Name of the object file you are uploading


```python
# Creating the Bucket
s3.create_bucket(Bucket='example1811')
```




    {'ResponseMetadata': {'RequestId': '70C242129784AA04',
      'HostId': 'INZeeM11oVim4YDUOwg/A5gd3t4MhsVeAwvqhy43OdDDRWVbtDdGhNGjauRABApLJmumHfvkW5w=',
      'HTTPStatusCode': 200,
      'HTTPHeaders': {'x-amz-id-2': 'INZeeM11oVim4YDUOwg/A5gd3t4MhsVeAwvqhy43OdDDRWVbtDdGhNGjauRABApLJmumHfvkW5w=',
       'x-amz-request-id': '70C242129784AA04',
       'date': 'Tue, 16 Jun 2020 23:51:30 GMT',
       'location': '/example1811',
       'content-length': '0',
       'server': 'AmazonS3'},
      'RetryAttempts': 0},
     'Location': '/example1811'}




```python
# Uploadin the file
s3.upload_file(Filename= r'C:\Users\shrey\OneDrive\Desktop\s3\sjk1.txt',
               Bucket='example1811',
               Key= 'sjk1.txt')
```

We can provide `ExtraArgs` parameter to set and assigns the canned ACL (access control list) value 'public-read' to the S3 object. This will make your file accessable publicly. All public accesses are blocked by default.


```python
s3.list_objects(Bucket='example1811',MaxKeys=2,Prefix='sjk')['Contents']
```




    [{'Key': 'sjk1.txt',
      'LastModified': datetime.datetime(2020, 6, 16, 23, 51, 31, tzinfo=tzutc()),
      'ETag': '"bff45de157e360bd40661a2d264f74a8"',
      'Size': 12,
      'StorageClass': 'STANDARD',
      'Owner': {'DisplayName': 'Gandhi.ka',
       'ID': 'e9255dc22989ff9f56bb221d1755f1397e0f4d25c37f682aa7b9136a3b8a5417'}}]



`Prefix` parameter is to select multiple files with similarity in names

### Getting object metadata:
This plays important role in accessing data from private buckets that we will see ahead in this Notebook


```python
response = s3.head_object(Bucket='example1811',Key='sjk1.txt')
print(response)
```

    {'ResponseMetadata': {'RequestId': 'FEFFCC49AB85098F', 'HostId': '0RwM3oELZ6FYwaCtsbpmKg8t2pfXxrg/RPjSEC6coeFmQdyTVseCD8jEjpiOagNTS5tThvdGz90=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': '0RwM3oELZ6FYwaCtsbpmKg8t2pfXxrg/RPjSEC6coeFmQdyTVseCD8jEjpiOagNTS5tThvdGz90=', 'x-amz-request-id': 'FEFFCC49AB85098F', 'date': 'Wed, 17 Jun 2020 00:04:30 GMT', 'last-modified': 'Tue, 16 Jun 2020 23:51:31 GMT', 'etag': '"bff45de157e360bd40661a2d264f74a8"', 'accept-ranges': 'bytes', 'content-type': 'binary/octet-stream', 'content-length': '12', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'AcceptRanges': 'bytes', 'LastModified': datetime.datetime(2020, 6, 16, 23, 51, 31, tzinfo=tzutc()), 'ContentLength': 12, 'ETag': '"bff45de157e360bd40661a2d264f74a8"', 'ContentType': 'binary/octet-stream', 'Metadata': {}}


### Downloading Objects from S3 Bucket:
This function is very similar to `upload_file` function as they have similar parameters.


```python
s3.download_file(Filename= r'C:\Users\shrey\OneDrive\Desktop\s3\sjk1.txt',
               Bucket='example1811',
               Key= 'sjk1.txt')
```

### Copying objects:
We can copy objects from one S3 bucket to another using Boto3


```python
s3.list_buckets()['Buckets']
```




    [{'Name': 'example1811',
      'CreationDate': datetime.datetime(2020, 6, 16, 23, 51, 30, tzinfo=tzutc())},
     {'Name': 'example1811b',
      'CreationDate': datetime.datetime(2020, 6, 17, 1, 16, 45, tzinfo=tzutc())}]




```python
CopySource = {'Bucket' : 'example1811',
             'Key':'sjk1.txt'}
```


```python
s3.copy_object(Bucket="example1811b", CopySource = CopySource, Key="sjk1.txt")
```




    {'ResponseMetadata': {'RequestId': '19B971F8D5491465',
      'HostId': 'GLZb7veZFmFaKTIPPuC+V+IpIWJ1gU/H4zEz17Wo+U9yJl0Wr6FO52x83IkX8xJ7aFs9lM45y5I=',
      'HTTPStatusCode': 200,
      'HTTPHeaders': {'x-amz-id-2': 'GLZb7veZFmFaKTIPPuC+V+IpIWJ1gU/H4zEz17Wo+U9yJl0Wr6FO52x83IkX8xJ7aFs9lM45y5I=',
       'x-amz-request-id': '19B971F8D5491465',
       'date': 'Wed, 17 Jun 2020 01:27:05 GMT',
       'content-type': 'application/xml',
       'content-length': '234',
       'server': 'AmazonS3'},
      'RetryAttempts': 0},
     'CopyObjectResult': {'ETag': '"bff45de157e360bd40661a2d264f74a8"',
      'LastModified': datetime.datetime(2020, 6, 17, 1, 27, 5, tzinfo=tzutc())}}



## Keeping objects secure:
This topic is very important. Companies have started using AWS to store their confidential data on cloud and it would not have been possible without aws's exceptional security.
It is a best practice to take all the steps in keeping your bucket private giving access permissions to only required users and IP addresses.

If you keep your bucket publically available it is as simple as putting this link "https://example1811.s3.amazonaws.com/sjk1811.txt" in your browser to access all the confidential data from anywhere in globe.


There are 4 main Access settings for these S3 Buckets:
- IAM Roles
- ACL
- Bucket Policy
- Presigned URL

<p style="text-align: center;"><img src="images/aws_s3_boto3/security.png"/></p>

**Lets try making Object sjk.txt public for an example**


```python
s3.put_object_acl(Bucket='example1811', Key='sjk1.txt', ACL='public-read')
```




    {'ResponseMetadata': {'RequestId': 'DBD31162FEEB8CB5',
      'HostId': 'G+8ZhQ8wJfDKmXe1cdXFvOb60MohBQ/b4QLAEwnxbhAKkLP/8STudBP7W59zKqUPnLuo9aW9kSY=',
      'HTTPStatusCode': 200,
      'HTTPHeaders': {'x-amz-id-2': 'G+8ZhQ8wJfDKmXe1cdXFvOb60MohBQ/b4QLAEwnxbhAKkLP/8STudBP7W59zKqUPnLuo9aW9kSY=',
       'x-amz-request-id': 'DBD31162FEEB8CB5',
       'date': 'Wed, 17 Jun 2020 00:23:48 GMT',
       'content-length': '0',
       'server': 'AmazonS3'},
      'RetryAttempts': 0}}



**My nightmare has come true!!!** Just putting this link in my browser https://example1811.s3.amazonaws.com/sjk1811.txt downloads the file on my besktop.
You definetly do not want to be this guy.


```python
s3.put_object_acl(Bucket='example1811', Key='sjk1.txt', ACL='private')
```




    {'ResponseMetadata': {'RequestId': 'D4A15FA4470BE116',
      'HostId': 'YBHRXq9XB0rF/DJSxpua4OaYY0ybkFOfhy9ol6u4+wm24zIqrFTQ26JIhzt1UYn9saFPe3U1u6E=',
      'HTTPStatusCode': 200,
      'HTTPHeaders': {'x-amz-id-2': 'YBHRXq9XB0rF/DJSxpua4OaYY0ybkFOfhy9ol6u4+wm24zIqrFTQ26JIhzt1UYn9saFPe3U1u6E=',
       'x-amz-request-id': 'D4A15FA4470BE116',
       'date': 'Wed, 17 Jun 2020 00:26:12 GMT',
       'content-length': '0',
       'server': 'AmazonS3'},
      'RetryAttempts': 0}}



### Setting ACLs on uploading the files:
You can use `ExtraArgs` to set ACLs right when you upload the file.


```python
s3.upload_file(Bucket='example1811',Filename='sjk1.csv',
               Key='sjk1.csv',
               ExtraArgs={'ACL':'public-read'})
```

### How to access these private files?

This is not some sort of trick, you need body information of the object which is not possible without having the keys.


```python
import pandas as pd
```


```python
# getting obj information to retrieve `body`
obj = s3.get_object(Bucket='example1811', Key='sjk1.txt')
```


```python
obj['Body']
```




    <botocore.response.StreamingBody at 0x280195ef550>




```python
df = pd.DataFrame('<botocore.response.StreamingBody at 0x280195ef550>')
```

**There you have a Access to the file without any hussle**

### Pre-signed URLs
The presigned URLs are valid only for the specified duration. Anyone who receives the presigned URL can then access the object.
- The expiring time is in seconds as per ExpiresIn parameter provided.
- We use `Params` to provide information about what object from what bucket we want to give url access to for the given time.


```python
share_url = s3.generate_presigned_url(ClientMethod='get_object',ExpiresIn=3600,
                                      Params={'Bucket': 'example1811',
                                              'Key': 'sjk1.txt'})

```

**We can use this link to access the file. The link expires in 3600 seconds.**


```python
share_url
```




    'https://example1811.s3.amazonaws.com/sjk1.txt?AWSAccessKeyId=AKIAQU6F5LRCKOWLDSTU&Signature=pZea4i%2BSakX68G43u1Tl3JnnuQc%3D&Expires=1592359332'



### Deleting objects:
It is very important to delete all the resources you built on aws if you are not using them anymore to avoid any form of charges.
- We need to empty the bucket before we delete it.


```python
s3.delete_object(Bucket='example1811',
    Key='sjk1.txt')
```




    {'ResponseMetadata': {'RequestId': '222E2934E2372524',
      'HostId': 'XR/2xPcYbuEAr7jyeN4b4X7e20YQYI8LG7E/P1z7119qkP/Ks9YzlhKu5Ik0cpjuoQcrhAK2lSY=',
      'HTTPStatusCode': 204,
      'HTTPHeaders': {'x-amz-id-2': 'XR/2xPcYbuEAr7jyeN4b4X7e20YQYI8LG7E/P1z7119qkP/Ks9YzlhKu5Ik0cpjuoQcrhAK2lSY=',
       'x-amz-request-id': '222E2934E2372524',
       'date': 'Wed, 17 Jun 2020 01:47:25 GMT',
       'server': 'AmazonS3'},
      'RetryAttempts': 0}}




```python
s3.delete_object(Bucket='example1811b',
    Key='sjk1.txt')
```




    {'ResponseMetadata': {'RequestId': 'A8856DAE040AA7CA',
      'HostId': 'IcT8TtPJcUMyYMoaPEqT2ioKhIH8r24GW4iTYSxq85N+cmF+mrNW/0f3sff9ZM43uLO3iaEHYYI=',
      'HTTPStatusCode': 204,
      'HTTPHeaders': {'x-amz-id-2': 'IcT8TtPJcUMyYMoaPEqT2ioKhIH8r24GW4iTYSxq85N+cmF+mrNW/0f3sff9ZM43uLO3iaEHYYI=',
       'x-amz-request-id': 'A8856DAE040AA7CA',
       'date': 'Wed, 17 Jun 2020 01:47:36 GMT',
       'server': 'AmazonS3'},
      'RetryAttempts': 0}}




```python
s3.delete_bucket(Bucket = 'example1811')
```




    {'ResponseMetadata': {'RequestId': '6D260CFDD753F8EF',
      'HostId': 'AVUsI4zyE5U6xSziBLNWxsmYkLxGMyl+PcxGIWYndMk5X6C4QC9h9nyb2D23WaSnqrOk4aVg7T0=',
      'HTTPStatusCode': 204,
      'HTTPHeaders': {'x-amz-id-2': 'AVUsI4zyE5U6xSziBLNWxsmYkLxGMyl+PcxGIWYndMk5X6C4QC9h9nyb2D23WaSnqrOk4aVg7T0=',
       'x-amz-request-id': '6D260CFDD753F8EF',
       'date': 'Wed, 17 Jun 2020 01:47:41 GMT',
       'server': 'AmazonS3'},
      'RetryAttempts': 0}}




```python
s3.delete_bucket(Bucket = 'example1811b')
```




    {'ResponseMetadata': {'RequestId': '5BB65A86547A9A5D',
      'HostId': 'jja2+sU/VpCrofKA4afVTi5H/Ec3d7wgyAMx2jo0+nHP6RJTIU29sCiPImCe8yTUllUCwhMXbqo=',
      'HTTPStatusCode': 204,
      'HTTPHeaders': {'x-amz-id-2': 'jja2+sU/VpCrofKA4afVTi5H/Ec3d7wgyAMx2jo0+nHP6RJTIU29sCiPImCe8yTUllUCwhMXbqo=',
       'x-amz-request-id': '5BB65A86547A9A5D',
       'date': 'Wed, 17 Jun 2020 01:47:48 GMT',
       'server': 'AmazonS3'},
      'RetryAttempts': 0}}




```python

```
